# -*- coding: utf-8 -*-
"""yelp-reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-SdcA8XJhbOBrwjGMuaaTm2b-xG1jMIN
"""

# !pip install torch
# !pip install transformers
# !pip install sentencepiece
# !pip install bertopic
# !pip install openai
# import locale
# def getpreferredencoding(do_setlocale = True):
#     return "UTF-8"
# locale.getpreferredencoding = getpreferredencoding

# !python -m spacy download en_core_web_md
import openai
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F

from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from hdbscan import HDBSCAN
import nltk.stem


"""Copy this file to your Drive [yelp-business](https://drive.google.com/file/d/1-BDL1vFmbksIoZ1hAlw_WBkzv5hhKns0/view?usp=sharing), [yelp-reviews](https://drive.google.com/file/d/1-JL6b_jkO-YZ0J209cAyty0bzNoV_Xtx/view?usp=sharing). We also have a mapping file to map business categories into [yelp-reviews](https://drive.google.com/file/d/1-JL6b_jkO-YZ0J209cAyty0bzNoV_Xtx/view?usp=sharing) """

business=pd.read_csv("drive/MyDrive/yelp_business.csv")
reviews=pd.read_csv("drive/MyDrive/yelp_reviews.csv")

dfbusiness_cats = ''.join(business['categories'].astype('str'))
#one category type per row
bcats=pd.DataFrame(dfbusiness_cats.split(','),columns=['categories'])
x=bcats.categories.value_counts()
x=x.sort_values(ascending=False)
#x=x.iloc[0:20]

plt.figure(figsize=(16,4))
ax = plt.bar(x=x.index[0:20],height=x.values[0:20])
plt.title("What are the top business categories for the Total Dataset?")
a=plt.xticks(rotation=45)

x.to_csv("Imp_topics.csv")

"""Manually mapped 200 of the top categories. We found that most businesses listed on yelp are food related"""

cat=pd.read_csv("/content/drive/MyDrive/Imp_topics_manually_updated.csv",encoding='latin-1')

def categorize(items:list):
  
  items=[i.strip() for i in items]
  res={}
  for i in items:
    if i in cat['Type'].to_list():
      val=cat[cat['Type']==i]['Category'].values[0]
      res[val]=1+res.get(val,0)
    else:
      continue
  return sorted(res.items(),key=lambda x:x[1],reverse=True)[0][0] if len(res)>0 else 'Others'

business['segment']=business['categories'].apply(lambda x:categorize(str(x).split(',')))

merged_df=reviews.merge(business,left_on='business_id',right_on='business_id',how='inner')
merged_df=merged_df.rename({'stars_y':'business_rating','stars_x':'review_rating'},axis=1)

"""Now the task is to find the indivual topics in each review. There are multiple techniques that we can use, but BERTopic, a topic modelling system built on BERT gives us our best set of topics.

Given that each business segment is different, we perform indivudal topic modelling to get customized topics for type of business i.e. topics for restaurants will differ from bars
"""

merged_df['segment'].value_counts()

restaurants=merged_df[merged_df['segment']=='Restaurant'].reset_index().drop(['index'],axis=1)

import nltk
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
wn = nltk.WordNetLemmatizer()

from bertopic import BERTopic
# Dimension reduction
from umap import UMAP

stopwords = nltk.corpus.stopwords.words('english')

restaurants['text_n_sw']=restaurants['text'].apply(lambda x: ' '.join([w for w in x.split() if w.lower() not in stopwords]))

restaurants['review_lemmatized'] = restaurants['text_n_sw'].apply(lambda x: ' '.join([wn.lemmatize(w) for w in x.split() if w not in stopwords]))

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

english_stemmer = nltk.stem.SnowballStemmer('english') 

class StemmedCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self).build_analyzer()
        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])

def train_bert(docs):
    embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

    cluster_model = HDBSCAN(min_cluster_size = 15, 
                            metric = 'euclidean', 
                            cluster_selection_method = 'eom', 
                            prediction_data = True)
    
    ctfidf_model = ClassTfidfTransformer(bm25_weighting=True)                         
    vectorizer_model = StemmedCountVectorizer(analyzer="word",stop_words="english", ngram_range=(1, 2))

    # BERTopic model
    topic_model = BERTopic(embedding_model = embedding_model,
                           hdbscan_model = cluster_model,
                           ctfidf_model=ctfidf_model,
                           vectorizer_model=vectorizer_model,
                           language="english",verbose=True,nr_topics=10)

    # Fit the model on review text
    topics, probs = topic_model.fit_transform(docs)
    return topic_model

topic_model = train_bert(restaurants['text'])

topics=topic_model.get_topic_info()

topics['list']=topics['Topic'].apply(lambda x:topic_model.get_topic(x))

topics.to_csv("individual_topics.csv")

"""1. Food and dining experience
2. Customer experience at a restaurant
3. Yelp reviews and their impact on businesses
4. Food and dining experience at various restaurant chains
5. Quality of dishes at a restaurant
"""
#However, we want individual topics within each review. For that, we performed spacy summarization to get the topics within each review


import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from collections import defaultdict


attributes = {
    "food": ["food", "cuisine", "dish", "flavor","fresh"],
    "service": ["service", "waitstaff", "server", "staff","manager"],
    "ambiance": ["ambiance", "atmosphere", "decor", "music","place","seat"],
    "value": ["value", "price", "cost", "affordability"],
    "location": ["nearby", "address", "parking", "accessibility","locality","neighbourhood"]
}


def get_similar_attribute(word, threshold=0.5):
    max_similarity = 0
    most_similar_attribute = None

    for attribute, attribute_words in attributes.items():
        for attribute_word in attribute_words:
            similarity = nlp(word).similarity(nlp(attribute_word))
            if similarity >= threshold and similarity > max_similarity:
                max_similarity = similarity
                most_similar_attribute = attribute_word
    return most_similar_attribute

def get_summary(text, num_sentences=2):
    doc = nlp(text.lower())
    sentences = [str(sent).strip() for sent in doc.sents]
    word_frequencies = defaultdict(float)
    for word in doc:
        if word.text not in STOP_WORDS:
            word_frequencies[word.text] += 1
    max_frequency = max(word_frequencies.values())

    for word in word_frequencies.keys():
        word_frequencies[word] /= max_frequency
    sentence_scores = defaultdict(float)
    
    for sent in sentences:
        for word in nlp(sent):
            if word.text in word_frequencies.keys():
                sentence_scores[sent] += word_frequencies[word.text]
    top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]
    
    summary = [sentence[0] for sentence in top_sentences]
    
    return ' '.join(summary)

def get_topics_2(review_text):
  summary_text = get_summary(review_text)
  mentioned_attributes = set()
  for word in summary_text.split():
      attribute = get_similar_attribute(word)
      if attribute is not None:
          mentioned_attributes.add(attribute)

  return mentioned_attributes

restaurants['topics_fnl']=restaurants['text'].apply(lambda x: get_topics_2(x))


from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("yangheng/deberta-v3-base-absa-v1.1")
model = AutoModelForSequenceClassification.from_pretrained("yangheng/deberta-v3-base-absa-v1.1")
restaurants=pd.read_csv("drive/MyDrive/reviews_w_fnl_topics.csv")

def get_sentiments(sentence,attrs):
  #sentence = restaurants['text'][0]
  # print(f"Sentence: {sentence}")
  # print()

  attrs=attrs.split(' | ')
  result={}
  for i in attrs:
    aspect = i
    inputs = tokenizer(f"[CLS] {sentence} [SEP] {aspect} [SEP]",return_tensors="pt")
    outputs = model(**inputs)
    res=F.softmax(outputs.logits, dim=1).detach()[0]
    if(torch.argmax(res).item()==0):
      Sent="Negative"
    elif(torch.argmax(res).item()==1):
      Sent="Neutral"
    else:
      Sent="Positive"
    result[i]={Sent:max(res).item()}
  return result

restaurants['sentiments']=restaurants.apply(lambda x:get_sentiments(x['text'],x['topics_fnl']),axis=1)
restaurants.to_csv("drive/MyDrive/final_to_gpt.csv")